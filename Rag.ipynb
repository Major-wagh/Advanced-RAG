{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from redisvl.extensions.llmcache import SemanticCache\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Milvus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = (\n",
    "    OllamaEmbeddings(model=\"nomic-embed-text\",base_url=\"http://localhost:11434\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"./pdfs-master\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document splitting a in process\n",
      "Document splitted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'pdfs-master\\\\NVIDIA - Accelerated Computing and the Democratization of Supercomputing Technical Overview (sc18-tesla-democratization-tech-overview-r4-web).pdf', 'page': 1}, page_content='ACCELERATED COMPUTING AND THE DEMOCRATIZATION OF SUPERCOMPUTING | TECHNICAL OVERVIEW | 2Table of Contents:\\nThe Accelerated Data Center \\nOptimizing Data Center Productivity\\nSame Throughput with Fewer Server Nodes\\nAren’t GPU-Accelerated Servers More Expensive? \\nWhat About Operational Cost?\\nWhat if the CPU is Given Away at No Cost?\\nMaximizing Budget and Throughput\\nLower Cost with  Acceleration\\nDemocratizing the Supercomputer3\\n6\\n7\\n8\\n10\\n11\\n12\\n13\\n14')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter= RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len\n",
    ")   \n",
    "\n",
    "print(\"Document splitting a in process\")\n",
    "docs = text_splitter.split_documents(docs)\n",
    "print(\"Document splitted\")\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start connecting to Milvus\n"
     ]
    }
   ],
   "source": [
    "MILVUS_PORT=\"19530\"\n",
    "MILVUS_HOST=\"127.0.0.1\"\n",
    " \n",
    "print(\"start connecting to Milvus\")\n",
    "vector_db = Milvus.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    connection_args={\"host\": MILVUS_HOST ,\"port\": MILVUS_PORT,\"db_name\":\"default\"},\n",
    "    collection_name=\"NvidiaPDF\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting milvus for context retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start connecting to Milvus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TUF GAMING\\miniconda3\\envs\\rag\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `Milvus` was deprecated in LangChain 0.2.0 and will be removed in 0.3.0. An updated version of the class exists in the langchain-milvus package and should be used instead. To use it run `pip install -U langchain-milvus` and import as `from langchain_milvus import MilvusVectorStore`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "MILVUS_PORT=\"19530\"\n",
    "MILVUS_HOST=\"127.0.0.1\"\n",
    " \n",
    "print(\"start connecting to Milvus\")\n",
    "vector_db = Milvus(\n",
    "    embeddings,\n",
    "    connection_args={\"host\": MILVUS_HOST ,\"port\": MILVUS_PORT,\"db_name\":\"default\"},\n",
    "    collection_name=\"NvidiaPDF\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TUF GAMING\\miniconda3\\envs\\rag\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:21:33\u001b[0m \u001b[34mredisvl.index.index\u001b[0m \u001b[1;30mINFO\u001b[0m   Index already exists, not overwriting.\n"
     ]
    }
   ],
   "source": [
    "llmcache = SemanticCache(\n",
    "    name=\"PDFs\",\n",
    "    prefix=\"PDFs\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    distance_threshold=0.1\n",
    ")\n",
    "llmcache.set_threshold(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:21:34\u001b[0m \u001b[34m[RedisVL]\u001b[0m \u001b[1;30mINFO\u001b[0m   Indices:\n",
      "\u001b[32m10:21:34\u001b[0m \u001b[34m[RedisVL]\u001b[0m \u001b[1;30mINFO\u001b[0m   1. PDFs\n"
     ]
    }
   ],
   "source": [
    "!rvl index listall --host localhost --port 6379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "langfuse_handler = CallbackHandler(\n",
    "    public_key=\"pk-lf-bf97c47d-6b38-492a-a4e3-5faf52310023\",\n",
    "    secret_key=\"sk-lf-e0701d57-6d12-432b-926a-6b9c6bc4502a\",\n",
    "    host=\"http://localhost:3000\",\n",
    "    user_id=\"PDFs\"\n",
    ")\n",
    "\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-e0701d57-6d12-432b-926a-6b9c6bc4502a\"\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-bf97c47d-6b38-492a-a4e3-5faf52310023\"\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse_handler = CallbackHandler()\n",
    "langfuse = Langfuse()\n",
    "\n",
    "langfuse_handler.auth_check()\n",
    "langfuse.auth_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_router import Route\n",
    "\n",
    "greeting_route = Route(\n",
    "    name=\"greet_user\",\n",
    "    utterances=[\n",
    "        \"hello\",\n",
    "        \"hi\",\n",
    "        \"hey\",\n",
    "        \"Whatsupppppp\",\n",
    "        \"good morning\",\n",
    "        \"good afternoon\",\n",
    "        \"good evening\",\n",
    "    ],\n",
    ")\n",
    "#    time_route, \n",
    "routes = [\n",
    "    greeting_route, \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sentence-transformers/all-mpnet-base-v2 ; Score: 0.3\n"
     ]
    }
   ],
   "source": [
    "from semantic_router import RouteLayer\n",
    "from semantic_router.encoders import HuggingFaceEncoder\n",
    "encoder = HuggingFaceEncoder()\n",
    "name: str = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "type: str = \"huggingface\"\n",
    "score_threshold: float = 0.3\n",
    "encoder.score_threshold=0.3\n",
    "encoder.name=name\n",
    "print(f\"Name: {encoder.name} ; Score: {encoder.score_threshold}\")\n",
    "rl = RouteLayer(encoder=encoder, routes=routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sentence-transformers/all-mpnet-base-v2 ; Score: 0.3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Name: {encoder.name} ; Score: {encoder.score_threshold}\")\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n ------------\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3\",base_url=\"http://localhost:11434\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import  RunnableParallel, RunnablePassthrough\n",
    " \n",
    " \n",
    "# Updated prompt template to include history and context\n",
    "rag_template = \"\"\"**Welcome to PDF Chat Assistant!**\n",
    " \n",
    "I'm a Chatbot, your dedicated AI assistant for PDF summarization\n",
    "\n",
    " \n",
    "**Please remember:**\n",
    " \n",
    "* I value professionalism and confidentiality in our interactions. \n",
    "* If unsure, I will say I don't know and ask to give more information in the query. If the query is irrelevant, I will say it's beyond my area of expertise.\n",
    "\n",
    "**Context:**\n",
    "{context}\n",
    "**Query:**\n",
    "{question}\n",
    "**Answer:**\n",
    "\"\"\"\n",
    " \n",
    "# Updating the prompt template\n",
    "review_template = ChatPromptTemplate.from_template(rag_template)\n",
    " \n",
    "# Creating the retriever with formatting\n",
    "conversation_chain =(RunnableParallel(context=retriever | format_docs, question=RunnablePassthrough()) \n",
    "    | review_template\n",
    "    | llm \n",
    "    | StrOutputParser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    " \n",
    "def get_time():\n",
    "    now = datetime.now()\n",
    "    return (\n",
    "        f\"The current time is {now.strftime('%H:%M')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_layer(question: str):\n",
    "    route = rl(question)\n",
    "    if route.name == \"get_time\":\n",
    "        question += f\" (SYSTEM NOTE: {get_time()})\"\n",
    "    elif route.name == \"greet_user\":\n",
    "        question += f\" (SYSTEM NOTE: Say Hello I am PDF chatbot)\"\n",
    "    else:\n",
    "        query1 = question + f\" (SYSTEM NOTE: pdf query) \"\n",
    "        results = llmcache.check(prompt=question)\n",
    "        if results:\n",
    "            question = \"###Cache hit: \" + query1 + \" \" + results[0][\"response\"]\n",
    "        else:\n",
    "            response = conversation_chain.invoke(question, config={\"callbacks\": [langfuse_handler]})\n",
    "            llmcache.store(prompt=question, response=response)\n",
    "            question = \"###Cache miss \" + query1 + \" \" + response\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Cache hit: what is Nvidia DGX-1 (SYSTEM NOTE: pdf query)  The NVIDIA DGX-1 is a powerful system built with NVIDIA Pascal-powered Tesla P100 accelerators, featuring a massive increase in GPU memory capacity. It's designed for deep learning and AI-acceleration, with a complete software platform that includes major deep learning frameworks, GPU training systems, and GPU drivers.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"what is Nvidia DGX-1\"\"\"\n",
    " \n",
    "sr_query = semantic_layer(question)\n",
    "print(sr_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For more information on NVIDIA DGX-1, visit www.nvidia.com/dgx1\n",
      "© 2016 NVIDIA Corporation. All rights reserved. NVIDIA, the NVIDIA logo, and Iray are trademarks and/or registered trademarks of NVIDIA \n",
      "Corporation. All company and product names are trademarks or registered trademarks of the respective owners with which they are \n",
      "associated. Features, pricing, availability, and specifications are all subject to change without notice. DeC16computing for Infinite Opportunities \n",
      "The NVIDIA DGX-1 is the first system built with \n",
      "NVIDIA Pascal™-powered Tesla® P100 accelerators. \n",
      "The NVIDIA NVLink™ implementation delivers a \n",
      "massive increase in GPU memory capacity, giving you \n",
      "a system that can learn, see, and simulate our world.  \n",
      "Analyze. Visualize. AI-Accelerate  \n",
      "The NVIDIA DGX-1 software stack includes major \n",
      "deep learning frameworks, the NVIDIA DIGITS™ GPU \n",
      "training system, the NVIDIA Deep Learning SDK \n",
      "(e.g. CuDNN, NCCL), NVIDIA Docker, GPU drivers,\n",
      "\n",
      " ------------NVIDIA DGX-1 With Tesla V100 System Architecture  WP-08437-002_v01 | 386.3 Conclusion\n",
      "Powerful Volta GPUs and high-per formance NVLink interconnect ar e just part of the DGX-1 story. More \n",
      "productivity and performance ben efits come from the fact that D GX-1 is an integrated system, with a \n",
      "complete software platform aimed at deep learning, as described  in Sections 4 and 5. This includes the \n",
      "deep learning framework optimiza tions such as those in NVCaffe,  cuBLAS, cuDNN, and other GPU-\n",
      "accelerated libraries, and NVLink-tuned collective communicatio ns through NCCL. This integrated \n",
      "software platform, combined with Tesla V100 and NVLink, ensures  that DGX-1 outperforms similar off-\n",
      "the-shelf systems.\n",
      "To learn more about NVIDIA DGX-1, visit http://www.nvidia.com/dgx1.\n",
      "\n",
      " ------------through the same NVIDIA Docker containers and NVIDIA Container Registry service that are used \n",
      "for all DGX platforms. This single un ified deep learning stack simplifies workflows. Data scientists \n",
      "can now easily scale their work and deploy the solutions developed with DGX Station onto DGX -1 \n",
      "servers in the data center or the NVIDIA Deep Learning Cloud.  \n",
      "Equally important and noteworthy, with NVIDIA maintaining and delivering the software stack, \n",
      "data scientists can now focus purely on training and deploying their deep learning solutions, and \n",
      "not waste time tuning and updating software components. The thousands of dollars of potential \n",
      "cost savings from improved productivity and better utilization of scarce deep learning expertise \n",
      "dwarfs the initial cost of the hardware.  \n",
      "KICKSTARTING AI INIT IATIVES  \n",
      "NVIDIA DGX Station is designed to kickstart the AI initiatives of individual researchers or\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(question)\n",
    "context = \"\\n\\n ------------\".join(doc.page_content for doc in docs)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TUF GAMING\\miniconda3\\envs\\rag\\Lib\\site-packages\\lazy_loader\\__init__.py:202: RuntimeWarning: subpackages can technically be lazily loaded, but it causes the package to be eagerly loaded even if it is already lazily loaded.So, you probably shouldn't use subpackages with this lazy feature.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from uptrain import EvalLLM, Evals, Settings\n",
    " \n",
    "import time\n",
    " \n",
    "def evaluate_uptrain(\n",
    "    question, \n",
    "    context, \n",
    "    response, \n",
    "    langfuse_handler\n",
    "):\n",
    "    data = [{\"question\": question, \"context\": context, \"response\": response}]\n",
    "    settings = Settings(\n",
    "        model='ollama/llama3', \n",
    "    )\n",
    "    eval_llm = EvalLLM(settings)\n",
    "    res = eval_llm.evaluate(\n",
    "        data=data,\n",
    "        project_name=\"Pdf_chat\",\n",
    "        checks=[\n",
    "            Evals.RESPONSE_COMPLETENESS, \n",
    "            Evals.RESPONSE_CONCISENESS, \n",
    "            Evals.RESPONSE_RELEVANCE,\n",
    "            Evals.VALID_RESPONSE,\n",
    "            Evals.RESPONSE_CONSISTENCY, \n",
    "            Evals.CONTEXT_RELEVANCE,\n",
    "            Evals.RESPONSE_COMPLETENESS_WRT_CONTEXT,\n",
    "            Evals.FACTUAL_ACCURACY,\n",
    "            Evals.CODE_HALLUCINATION,\n",
    "            Evals.PROMPT_INJECTION\n",
    "        ]\n",
    "    )\n",
    "    trace = langfuse.trace(name=\"METRICS EVALUATION\",user_id=\"PDFs\")\n",
    "    trace.span(name=\"retrieval\", input={'question': question}, output={'context': context})\n",
    "    trace.span(name=\"generation\", input={'question': question, 'context': context}, output={'response': response})\n",
    "    df = pd.DataFrame(res)\n",
    "    for _, row in df.iterrows():\n",
    "        for metric_name in [\n",
    "            \"response_completeness\", \"response_conciseness\", \"response_relevance\", \n",
    "            \"valid_response\", \"response_consistency\", \"response_completeness_wrt_context\", \n",
    "            \"factual_accuracy\", \"prompt_injection\", \"context_relevance\", \"factual_accuracy\", \n",
    "            \"code_hallucination\"\n",
    "        ]:\n",
    "            trace.score(name=metric_name, value=res[0][\"score_\"+metric_name])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-14 10:30:29.011\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 1/1 [03:14<00:00, 194.49s/it]\n",
      "c:\\Users\\TUF GAMING\\miniconda3\\envs\\rag\\Lib\\site-packages\\uptrain\\operators\\language\\llm.py:271: RuntimeWarning: coroutine 'LLMMulticlient.async_fetch_responses' was never awaited\n",
      "  with ThreadPoolExecutor(max_workers=1) as executor:\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "\u001b[32m2024-07-14 10:33:43.578\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 1/1 [01:45<00:00, 105.42s/it]\n",
      "\u001b[32m2024-07-14 10:35:29.003\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 1/1 [01:29<00:00, 89.09s/it]\n",
      "\u001b[32m2024-07-14 10:36:58.110\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 1/1 [01:47<00:00, 107.60s/it]\n",
      "\u001b[32m2024-07-14 10:38:45.721\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 1/1 [01:02<00:00, 62.87s/it]\n",
      "\u001b[32m2024-07-14 10:39:48.599\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 1/1 [02:46<00:00, 166.01s/it]\n",
      "\u001b[32m2024-07-14 10:42:34.621\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36muptrain.operators.language.response_quality\u001b[0m:\u001b[36mevaluate_local\u001b[0m:\u001b[36m499\u001b[0m - \u001b[31m\u001b[1mError when processing payload at index 0: None\u001b[0m\n",
      "\u001b[32m2024-07-14 10:42:34.647\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 1/1 [02:44<00:00, 164.62s/it]\n",
      "\u001b[32m2024-07-14 10:45:19.285\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 1/1 [03:09<00:00, 189.08s/it]\n",
      "\u001b[32m2024-07-14 10:48:28.370\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 1/1 [01:31<00:00, 91.17s/it]\n",
      "\u001b[32m2024-07-14 10:49:59.571\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 1/1 [03:32<00:00, 212.27s/it]\n",
      "\u001b[32m2024-07-14 10:53:31.881\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 1/1 [01:08<00:00, 68.36s/it]\n",
      "\u001b[32m2024-07-14 10:54:40.245\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 1/1 [01:05<00:00, 65.88s/it]\n"
     ]
    }
   ],
   "source": [
    "result_df = evaluate_uptrain(\n",
    "    question=question, \n",
    "    context=context, \n",
    "    response=sr_query, \n",
    "    langfuse_handler=langfuse_handler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
